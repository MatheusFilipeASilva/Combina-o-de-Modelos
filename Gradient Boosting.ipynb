{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1d165c-47c8-4d08-b204-0504a12130bd",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f699038-ff40-4d95-bd5f-7ddbf02dd528",
   "metadata": {},
   "source": [
    "### O que é o Gradinent Boosting?\n",
    "\n",
    "O gradient boosting é uma técnica de aprendizado de máquina de múltiplas iterações, cujo objetivo é realizar a previsão de uma variável, que pode ser uma categoria, e portanto um algoritmo de classificação, ou uma variável contínua, e, portanto, um algoritmo de regressão.\n",
    "\n",
    "### Embora ambos sejam algoritmos de Boosting, podemos listar muitas diferenças entre o Ada Boosting e o Gradient Boosting, vejamos algumas:\n",
    "* Ambos são baseados em árvores de regressão. No entanto, o Ada Boosting utiliza apenas stumps, enquanto o Gradient Boosting utiliza árvores completas\n",
    "* No Ada Boosting, cada árvore possui pesos diferentes para a decisão final baseado em seu índice de acerto. No Gradient Boosting, todas as árvores possuem igual impacto sobre a decisão final.\n",
    "* A atribuição da primeira previsão de um algoritmo Ada Boost é feita usando o primeiro stump, já a do Gradient Boosting é feita utilizando a média dos valores a serem previstos.\n",
    "* O Ada Boosting dificilmente overfitta em qualquer cenário, embora hajam exceções. Já o Gradient Boosting, caso não haja prudência ao selecionar o parâmetro Eta (learning_rate) é capaz de overfittar terrívelmente em 2 ou 3 iterações (para eta = 1, por exemplo).\n",
    "* Enquanto os algoritmos anteriores se preocupavam em prever especificamente a variável dependente, o Gradient Boosting se preocupa em prever o resíduo da etapa anterior, sendo a previsão da primeira etapa feita conforme descrito acima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9315367-aa2a-4d46-8871-c2d7ddc8e2be",
   "metadata": {},
   "source": [
    "### Implementação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4ec11c-f189-458a-98b2-65b3699262ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importanto bibliotecas:\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04fd5987-b184-4ddf-93f7-796da9faca36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Carregando dados:\n",
    "X, y = make_hastie_10_2(random_state=40028922)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Implementando o algoritmo:\n",
    "clf = HistGradientBoostingClassifier(max_iter = 100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f8fa3-7760-43f6-a986-8d49ea8dfc4e",
   "metadata": {},
   "source": [
    "### Implementação na dataset load_iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ed231c4-e4c4-41ac-8b17-ee8779aaa844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b9fb2d8-b86d-4d8b-a1f4-bc1343a1a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "irisX_train, irisX_test, irisy_train, irisy_test = train_test_split(X, y, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "021b70df-a01e-4ad3-a0fd-c941ce69009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfiris = GradientBoostingClassifier(n_estimators=120, random_state=40028922, validation_fraction=0.1).fit(irisX_train, irisy_train)\n",
    "clfiris.score(irisX_test, irisy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34091e79-59ea-4419-a647-e2d40f2e0bed",
   "metadata": {},
   "source": [
    "### Sobre nossa implementação acima, algumas coisas são notáveis:\n",
    "Utilizamos uma baixíssima quantidade de hiperparâmetros na nossa implementação. Seriam nossos hiper-parâmetros os melhores possíveis? Seria testar cada combinação manualmente uma opção viável na busca por um melhor modelo? A resposta é não. No entanto, existe uma ferramenta que permite que essa procura seja automática, que é o GridSearchCV. Assim, então, implementaremos novamente desta vez utilizando o GridSearch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39aa7301-be96-4b39-a782-528e2c05e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importando biblioteca:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_estim = list(range(50,250, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "536dfdd8-8da6-4860-8945-5bae9a0e6661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os melhores parâmetros encontrados foram:  {'max_depth': None, 'min_samples_split': 6, 'n_estimators': 70}\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': n_estim,\n",
    "    'max_depth': [None, 10, 20, 25],\n",
    "    'min_samples_split': [2,4,6,8,10,12]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(irisX_train, irisy_train)\n",
    "\n",
    "print('Os melhores parâmetros encontrados foram: ', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191e770-5a29-43c8-b179-228170627db4",
   "metadata": {},
   "source": [
    "### Assim sendo, avaliamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fced0b37-4d55-4bdd-8b20-c6c584cdac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A acurácia do nosso modelo otimizado pelo GridSearch foi:  0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "irisy_pred = grid_search.predict(irisX_test)\n",
    "print('A acurácia do nosso modelo otimizado pelo GridSearch foi: ', accuracy_score(irisy_test, irisy_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751895c-e469-48c4-a6c0-4a1a6e0d659d",
   "metadata": {},
   "source": [
    "### A diferença inexistente entre os scores se deve, provavelmente, a um misto de a base de dados ser muito pequena, e não haver diferença notável entre os parâmetros selecionados no nosso próprio modelo e o selecionado pelo GridSearch..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e129a392-ff6d-47f2-80cd-5a2390c1e9f3",
   "metadata": {},
   "source": [
    "### Hiper-parametros importantes do Gradient Boosting:\n",
    "* n_estimators: indica quantas tentativas recorrentes teremos de prever o erro que obtivemos na previsão anterior, ou seja, basicamente, indica o número de iterações do algoritmo.\n",
    "* validation_fraction: indica qual será o percentual da base utilizada para validação\n",
    "* n_iters_no_change: é um parâmetro usado para decidir se e sob qual condição as iterações devem ser interrompidas caso não haja alterações no score. Basicametne, estabelece um critério de parada precoce em caso de estagnação do score do algoritmo.\n",
    "* ccp_alpha: como de costume, é um parâmetro utilizado para estipular o grau de complexidade permitido em nossas árvores de regressão\n",
    "* learning_rate: é o nosso \"eta\" do GBoosting. Basicamente, esse parâmetro restringe a contribuição de cada iteração em direção do valor real, de modo a evitar uma convergência precoce e consequente overfitting.\n",
    "* verbose: cria uma descrição da execução do algoritmo, demonstrando as etapas que estão sendo executadas. É importante, principalmente para quando, por exemplo, o algoritmo estiver rodando em uma base de dados muito grande e sua execução for demasiado demorada, levantando dúvidas quanto à se o algoritmo ainda está em execução ou se aconteceu algum \"crash\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9da66-7244-4a57-a093-72e3b268e1cc",
   "metadata": {},
   "source": [
    "### Até agora, utilizamos o Gradient Boosting apenas em problemas de Classificação, mas ele também se aplica em regressão, como abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd4c338f-5226-4b3f-bd37-6ccd23b566cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importando bibliotecas:\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42008c1a-ee1d-47e0-a001-f33ebd677839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.748054670292009"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_friedman, y_friedman = make_friedman1(n_samples=1200, random_state=40028922, noise=1.0)\n",
    "trainX_friedman, testX_friedman = X_friedman[:200], X_friedman[200:]\n",
    "trainy_friedman, testy_friedman = y_friedman[:200], y_friedman[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=40028922, loss='squared_error'\n",
    ").fit(trainX_friedman, trainy_friedman)\n",
    "mean_squared_error(testy_friedman, est.predict(testX_friedman))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be34d8-dd5e-42aa-9666-a1b00dab86e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
