{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45bc46ae-3287-4372-8bf9-473d8e53823d",
   "metadata": {},
   "source": [
    "### Ada Boost, o que é?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebedd1-817f-4e2d-a22e-0840ce22aea2",
   "metadata": {},
   "source": [
    "O Ada Boost é uma técnica de aprendizado de máquina, que constitui-se da utilização de várias árvores de decisão para, combinando-as, deliberar sobre a natureza de uma determinada variável dependente com base em dados sobre variáveis independentes. Assim sendo, o Ada Boost lembra em muitos aspectos o Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f35787-a3bb-4890-ac96-b797eaf1cc69",
   "metadata": {},
   "source": [
    "### Quais as diferenças entre o Ada Boost e o Random Forest, então?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7196be1-7e19-45dc-ad26-6a310477bc3d",
   "metadata": {},
   "source": [
    "### Várias diferenças podem ser listadas entre o Ada Boost e o Random Forest, entre elas:\n",
    "-  O Random Forest utiliza-se de árvores com profundidades diversas, o Ada Boost utiliza apenas árvores de profundidade 1 (Stumps)\n",
    "-  O Random Forest atribui o mesmo peso à decisão de todas as árvores, enquanto, no Ada Boost, é atribuído um \"peso\" ao palpite de cada árvore com base em seu índice de acertos. Ou seja, a opinião de árvores que \"erram mais\" é menos relevante.\n",
    "-  No Ada Boost, uma árvore construída, influencia nas árvores que serão construídas subsequentemente, o que, no Random Forest, não acontece\n",
    "-  O adaboost se adapta (como o nome sugere - Adaptative Boosting) para aumentar o esforço de se prever especificamente os valores que as árvores anteriores não conseguiram prever. Isso, no entanto, torna o Ada Boost mais sujeito à overfiting.\n",
    "-  O Adaboost, quando realizado com um parametro n-estimators baixo, possui eficiência muito baixa, sendo pouco melhor do que um palpite aleatório. Isso se deve à profundidade das árvores de decisão ser fixada em 1.\n",
    "-  Comparado com o Random Forest, o Ada Boost tende a demorar mais para ser treinado, devido à característica de que os modelos são inter-dependentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f314dc5-47e0-4f15-9697-cd4fc2aac938",
   "metadata": {},
   "source": [
    "### Implementando o Ada Boost:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76957229-3adb-4e4e-8c85-9e2b95a16455",
   "metadata": {},
   "source": [
    "### Carregando bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05d9d9e2-3b68-4dfe-b77b-c9d49fc3cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6997dfc6-847a-4cd3-a177-626f4e68d397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Carregando dados:\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "### Treinando a árvore\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\")\n",
    "\n",
    "### Avaliando o resultado:\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6db60f0a-e77b-4055-b132-e35d358a9bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666667"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos ver se obtemos um resultado melhor aumentando o número de n_estimators:\n",
    "\n",
    "clf2 = AdaBoostClassifier(n_estimators=500, algorithm=\"SAMME\")\n",
    "\n",
    "### Avaliando o resultado:\n",
    "scores2 = cross_val_score(clf2, X, y, cv=5)\n",
    "scores2.mean()\n",
    "\n",
    "# Nossos resultados não melhoram! Provavelmente, o que está acontecendo é que ao realizar o treinamento com mais estimadores, o nosso modelo está\n",
    "# capturando ruídos, aumentando sua efetividade na base de treino, mas derrubando sua efetividade na base de testes. Ou seja, aumentar indiscriminada-\n",
    "# mente o número de árvores estimadoras não é sempre uma forma efetiva de melhorar o nosso modelo..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91156de-cdd3-4879-96b1-d5930387dcfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### O Ada Boost e seus hiper-parâmetros ###\n",
    "\n",
    "Assim como a maioria das funções de qualquer biblioteca do python, a implementação do Ada Boost via Scikit-learn aceita diversos hiper-parâmetros que customizam a execução da técnica. Os hiper-parâmetros especificam para o software de implementação determinadas características que queremos que a execução da técnica implementada possui. Alguns hiper-parâmetros do Ada Boost são:\n",
    "* n_estimators: este hiper-parâmetro especifica quantos stumps a nossa regressão Ada Boost terá. Ou seja, quantas árvores serão utilizadas (visto que todas as árvores utilizadas são stumps)\n",
    "* algorythm: especifica qual métodologia será usada, entre  \"SAMME\" e \"SAMME.R\". Por padrão, é SAMME. Se for SAMME, o modelo calculará as probabilidades de cada classe. Se for SAMME.R, ou seja, a probabilidade de cada classe é calculada, do contrário, não. O método SAMME.R está sinalizado como ultrapassado, e será removido em atualização futura, dada sua baixa eficiência em comparação com o método SAMME\n",
    "* learning_rate: este parâmetro especifica o quanto será exigido mais estimadores para gerar uma convergência, ou seja, diminui o \"peso\" individual de cada estimador conforme se aproxima de 0.\n",
    "* random_state: é uma espécie de \"semente de aleatoriedade\". Ou seja, ao especificarmos um random_state, tornamos possível que os nossos resultados possam ser replicados em outra máquina, embora os algoritmos envolvidos envolvam um alto grau de aleatoriedade\n",
    "* estimator: define qual algoritmo e quais características terá, o algoritmo que será utilizado na criação dos stumps. Por padrão, é o DecisionTreeClassifier.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc6152-8c2f-4ca4-b768-d0b3b777010f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
